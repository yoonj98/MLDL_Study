{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week35_Ridge(LB 0.41943).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJGVdDrQnWcx"
      },
      "source": [
        "## [Kaggle Clone Coding] Mercari Price Suggestion Challenge\n",
        "- [Mercari Price Suggestion Challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge/overview)\n",
        "- [Ridge (LB 0.41943)](https://www.kaggle.com/rumbok/ridge-lb-0-41944/data)\n",
        "  \n",
        "- Task : 제품 가격 추천 / 제안\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpwsGm9InjLq"
      },
      "source": [
        "### Kaggle API를 통해 코랩에 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "fRGrQxH-nNZ5",
        "outputId": "5dd2f013-f74c-474c-a9bc-07cc30fa0337"
      },
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e5f02c6-5d04-473e-9673-3b83cb12f7fd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e5f02c6-5d04-473e-9673-3b83cb12f7fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"yoonj98\",\"key\":\"dbb3b5607358d2775c1cb6107c3bd2d3\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jSf1AvnKnkhi",
        "outputId": "e5f12e13-7c25-456b-8bba-e436854f3bf8"
      },
      "source": [
        "import os \n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/Kaggle/kaggle/'\n",
        "os.mkdir('/content/drive/MyDrive/Kaggle/kaggle/')\n",
        "os.chdir('/content/drive/MyDrive/Kaggle/kaggle/')\n",
        "os.getcwd()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Kaggle/kaggle'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4P3B1junlLC",
        "outputId": "236d69b8-e9a3-4c1f-c947-fb54f49e7c84"
      },
      "source": [
        "# 다운받고자하는 대회의 data 탭에서 api 주소 가져오기\n",
        "! kaggle competitions download -c mercari-price-suggestion-challenge"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.tsv.7z to /content/drive/MyDrive/Kaggle/kaggle\n",
            " 85% 63.0M/74.3M [00:00<00:00, 112MB/s]\n",
            "100% 74.3M/74.3M [00:00<00:00, 116MB/s]\n",
            "Downloading test_stg2.tsv.zip to /content/drive/My Drive/Kaggle/kaggle\n",
            "100% 294M/294M [00:02<00:00, 137MB/s]\n",
            "100% 294M/294M [00:02<00:00, 122MB/s]\n",
            "Downloading sample_submission.csv.7z to /content/drive/My Drive/Kaggle/kaggle\n",
            "  0% 0.00/170k [00:00<?, ?B/s]\n",
            "100% 170k/170k [00:00<00:00, 21.9MB/s]\n",
            "Downloading test.tsv.7z to /content/drive/My Drive/Kaggle/kaggle\n",
            " 68% 23.0M/34.0M [00:00<00:00, 45.4MB/s]\n",
            "100% 34.0M/34.0M [00:00<00:00, 76.6MB/s]\n",
            "Downloading sample_submission_stg2.csv.zip to /content/drive/My Drive/Kaggle/kaggle\n",
            "100% 7.77M/7.77M [00:00<00:00, 79.6MB/s]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vepkYWzAnl-i",
        "outputId": "2a60b855-427d-40ee-b4dc-9c58251f04fb"
      },
      "source": [
        "# 데이터 확인\n",
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\t\t  sample_submission_stg2.csv.zip  test.tsv.7z\n",
            "sample_submission.csv.7z  test_stg2.tsv.zip\t\t  train.tsv.7z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs0AFCqanmhx",
        "outputId": "513da058-4f30-46cf-ed47-e3b70ddcc8b5"
      },
      "source": [
        "# 압축해제\n",
        "!p7zip -d test.tsv.7z\n",
        "!p7zip -d train.tsv.7z\n",
        "!p7zip -d sample_submission.csv.7z"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 35617013 bytes (34 MiB)\n",
            "\n",
            "Extracting archive: test.tsv.7z\n",
            "--\n",
            "Path = test.tsv.7z\n",
            "Type = 7z\n",
            "Physical Size = 35617013\n",
            "Headers Size = 122\n",
            "Method = LZMA2:24\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  8% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - test.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       154222160\n",
            "Compressed: 35617013\n",
            "\n",
            "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 77912192 bytes (75 MiB)\n",
            "\n",
            "Extracting archive: train.tsv.7z\n",
            "--\n",
            "Path = train.tsv.7z\n",
            "Type = 7z\n",
            "Physical Size = 77912192\n",
            "Headers Size = 122\n",
            "Method = LZMA2:24\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  3% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - train.tsv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       337809843\n",
            "Compressed: 77912192\n",
            "\n",
            "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 174228 bytes (171 KiB)\n",
            "\n",
            "Extracting archive: sample_submission.csv.7z\n",
            "--\n",
            "Path = sample_submission.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 174228\n",
            "Headers Size = 143\n",
            "Method = LZMA:12m\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       9595930\n",
            "Compressed: 174228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qake5xanu_5"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khUezzAAnvTj",
        "outputId": "f8ddabc4-1340-4838-c345-a5cefc1b5a8f"
      },
      "source": [
        "import multiprocessing as mp\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from scipy.sparse import csr_matrix\n",
        "import os\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import gc\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import re\n",
        "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
        "\n",
        "os.environ['MKL_NUM_THREADS'] = '4'\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "os.environ['JOBLIB_START_METHOD'] = 'forkserver'\n",
        "\n",
        "INPUT_PATH = r'/content/drive/MyDrive/Kaggle/kaggle/'\n",
        "\n",
        "\n",
        "def dameraulevenshtein(seq1, seq2):\n",
        "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
        "\n",
        "    This method has not been modified from the original.\n",
        "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
        "\n",
        "    This distance is the number of additions, deletions, substitutions,\n",
        "    and transpositions needed to transform the first sequence into the\n",
        "    second. Although generally used with strings, any sequences of\n",
        "    comparable objects will work.\n",
        "\n",
        "    Transpositions are exchanges of *consecutive* characters; all other\n",
        "    operations are self-explanatory.\n",
        "\n",
        "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
        "    lengths of the two sequences.\n",
        "\n",
        "    >>> dameraulevenshtein('ba', 'abc')\n",
        "    2\n",
        "    >>> dameraulevenshtein('fee', 'deed')\n",
        "    2\n",
        "\n",
        "    It works with arbitrary sequences too:\n",
        "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
        "    2\n",
        "    \"\"\"\n",
        "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
        "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
        "    # However, only the current and two previous rows are needed at once,\n",
        "    # so we only store those.\n",
        "    oneago = None\n",
        "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
        "    for x in range(len(seq1)):\n",
        "        # Python lists wrap around for negative indices, so put the\n",
        "        # leftmost column at the *end* of the list. This matches with\n",
        "        # the zero-indexed strings and saves extra calculation.\n",
        "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
        "        for y in range(len(seq2)):\n",
        "            delcost = oneago[y] + 1\n",
        "            addcost = thisrow[y - 1] + 1\n",
        "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
        "            thisrow[y] = min(delcost, addcost, subcost)\n",
        "            # This block deals with transpositions\n",
        "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
        "                    and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
        "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
        "    return thisrow[len(seq2) - 1]\n",
        "\n",
        "\n",
        "class SymSpell:\n",
        "    def __init__(self, max_edit_distance=3, verbose=0):\n",
        "        self.max_edit_distance = max_edit_distance\n",
        "        self.verbose = verbose\n",
        "        # 0: top suggestion\n",
        "        # 1: all suggestions of smallest edit distance\n",
        "        # 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
        "\n",
        "        self.dictionary = {}\n",
        "        self.longest_word_length = 0\n",
        "\n",
        "    def get_deletes_list(self, w):\n",
        "        \"\"\"given a word, derive strings with up to max_edit_distance characters\n",
        "           deleted\"\"\"\n",
        "\n",
        "        deletes = []\n",
        "        queue = [w]\n",
        "        for d in range(self.max_edit_distance):\n",
        "            temp_queue = []\n",
        "            for word in queue:\n",
        "                if len(word) > 1:\n",
        "                    for c in range(len(word)):  # character index\n",
        "                        word_minus_c = word[:c] + word[c + 1:]\n",
        "                        if word_minus_c not in deletes:\n",
        "                            deletes.append(word_minus_c)\n",
        "                        if word_minus_c not in temp_queue:\n",
        "                            temp_queue.append(word_minus_c)\n",
        "            queue = temp_queue\n",
        "\n",
        "        return deletes\n",
        "\n",
        "    def create_dictionary_entry(self, w):\n",
        "        '''add word and its derived deletions to dictionary'''\n",
        "        # check if word is already in dictionary\n",
        "        # dictionary entries are in the form: (list of suggested corrections,\n",
        "        # frequency of word in corpus)\n",
        "        new_real_word_added = False\n",
        "        if w in self.dictionary:\n",
        "            # increment count of word in corpus\n",
        "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
        "        else:\n",
        "            self.dictionary[w] = ([], 1)\n",
        "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
        "\n",
        "        if self.dictionary[w][1] == 1:\n",
        "            # first appearance of word in corpus\n",
        "            # n.b. word may already be in dictionary as a derived word\n",
        "            # (deleting character from a real word)\n",
        "            # but counter of frequency of word in corpus is not incremented\n",
        "            # in those cases)\n",
        "            new_real_word_added = True\n",
        "            deletes = self.get_deletes_list(w)\n",
        "            for item in deletes:\n",
        "                if item in self.dictionary:\n",
        "                    # add (correct) word to delete's suggested correction list\n",
        "                    self.dictionary[item][0].append(w)\n",
        "                else:\n",
        "                    # note frequency of word in corpus is not incremented\n",
        "                    self.dictionary[item] = ([w], 0)\n",
        "\n",
        "        return new_real_word_added\n",
        "\n",
        "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
        "        total_word_count = 0\n",
        "        unique_word_count = 0\n",
        "\n",
        "        for line in arr:\n",
        "            # separate by words by non-alphabetical characters\n",
        "            words = re.findall(token_pattern, line.lower())\n",
        "            for word in words:\n",
        "                total_word_count += 1\n",
        "                if self.create_dictionary_entry(word):\n",
        "                    unique_word_count += 1\n",
        "\n",
        "        print(\"total words processed: %i\" % total_word_count)\n",
        "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
        "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
        "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
        "        return self.dictionary\n",
        "\n",
        "    def create_dictionary(self, fname):\n",
        "        total_word_count = 0\n",
        "        unique_word_count = 0\n",
        "\n",
        "        with open(fname) as file:\n",
        "            for line in file:\n",
        "                # separate by words by non-alphabetical characters\n",
        "                words = re.findall('[a-z]+', line.lower())\n",
        "                for word in words:\n",
        "                    total_word_count += 1\n",
        "                    if self.create_dictionary_entry(word):\n",
        "                        unique_word_count += 1\n",
        "\n",
        "        print(\"total words processed: %i\" % total_word_count)\n",
        "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
        "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
        "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
        "        return self.dictionary\n",
        "\n",
        "    def get_suggestions(self, string, silent=False):\n",
        "        \"\"\"return list of suggested corrections for potentially incorrectly\n",
        "           spelled word\"\"\"\n",
        "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
        "            if not silent:\n",
        "                print(\"no items in dictionary within maximum edit distance\")\n",
        "            return []\n",
        "\n",
        "        suggest_dict = {}\n",
        "        min_suggest_len = float('inf')\n",
        "\n",
        "        queue = [string]\n",
        "        q_dictionary = {}  # items other than string that we've checked\n",
        "\n",
        "        while len(queue) > 0:\n",
        "            q_item = queue[0]  # pop\n",
        "            queue = queue[1:]\n",
        "\n",
        "            # early exit\n",
        "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
        "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
        "                break\n",
        "\n",
        "            # process queue item\n",
        "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
        "                if self.dictionary[q_item][1] > 0:\n",
        "                    # word is in dictionary, and is a word from the corpus, and\n",
        "                    # not already in suggestion list so add to suggestion\n",
        "                    # dictionary, indexed by the word with value (frequency in\n",
        "                    # corpus, edit distance)\n",
        "                    # note q_items that are not the input string are shorter\n",
        "                    # than input string since only deletes are added (unless\n",
        "                    # manual dictionary corrections are added)\n",
        "                    assert len(string) >= len(q_item)\n",
        "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
        "                                            len(string) - len(q_item))\n",
        "                    # early exit\n",
        "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
        "                        break\n",
        "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
        "                        min_suggest_len = len(string) - len(q_item)\n",
        "\n",
        "                # the suggested corrections for q_item as stored in\n",
        "                # dictionary (whether or not q_item itself is a valid word\n",
        "                # or merely a delete) can be valid corrections\n",
        "                for sc_item in self.dictionary[q_item][0]:\n",
        "                    if sc_item not in suggest_dict:\n",
        "\n",
        "                        # compute edit distance\n",
        "                        # suggested items should always be longer\n",
        "                        # (unless manual corrections are added)\n",
        "                        assert len(sc_item) > len(q_item)\n",
        "\n",
        "                        # q_items that are not input should be shorter\n",
        "                        # than original string\n",
        "                        # (unless manual corrections added)\n",
        "                        assert len(q_item) <= len(string)\n",
        "\n",
        "                        if len(q_item) == len(string):\n",
        "                            assert q_item == string\n",
        "                            item_dist = len(sc_item) - len(q_item)\n",
        "\n",
        "                        # item in suggestions list should not be the same as\n",
        "                        # the string itself\n",
        "                        assert sc_item != string\n",
        "\n",
        "                        # calculate edit distance using, for example,\n",
        "                        # Damerau-Levenshtein distance\n",
        "                        item_dist = dameraulevenshtein(sc_item, string)\n",
        "\n",
        "                        # do not add words with greater edit distance if\n",
        "                        # verbose setting not on\n",
        "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
        "                            pass\n",
        "                        elif item_dist <= self.max_edit_distance:\n",
        "                            assert sc_item in self.dictionary  # should already be in dictionary if in suggestion list\n",
        "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
        "                            if item_dist < min_suggest_len:\n",
        "                                min_suggest_len = item_dist\n",
        "\n",
        "                        # depending on order words are processed, some words\n",
        "                        # with different edit distances may be entered into\n",
        "                        # suggestions; trim suggestion dictionary if verbose\n",
        "                        # setting not on\n",
        "                        if self.verbose < 2:\n",
        "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
        "\n",
        "            # now generate deletes (e.g. a substring of string or of a delete)\n",
        "            # from the queue item\n",
        "            # as additional items to check -- add to end of queue\n",
        "            assert len(string) >= len(q_item)\n",
        "\n",
        "            # do not add words with greater edit distance if verbose setting\n",
        "            # is not on\n",
        "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
        "                pass\n",
        "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
        "                for c in range(len(q_item)):  # character index\n",
        "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
        "                    if word_minus_c not in q_dictionary:\n",
        "                        queue.append(word_minus_c)\n",
        "                        q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
        "\n",
        "        # queue is now empty: convert suggestions in dictionary to\n",
        "        # list for output\n",
        "        if not silent and self.verbose != 0:\n",
        "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
        "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "\n",
        "        # output option 1\n",
        "        # sort results by ascending order of edit distance and descending\n",
        "        # order of frequency\n",
        "        #     and return list of suggested word corrections only:\n",
        "        # return sorted(suggest_dict, key = lambda x:\n",
        "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
        "\n",
        "        # output option 2\n",
        "        # return list of suggestions with (correction,\n",
        "        #                                  (frequency in corpus, edit distance)):\n",
        "        as_list = suggest_dict.items()\n",
        "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
        "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
        "\n",
        "        if self.verbose == 0:\n",
        "            return outlist[0]\n",
        "        else:\n",
        "            return outlist\n",
        "\n",
        "        '''\n",
        "        Option 1:\n",
        "        ['file', 'five', 'fire', 'fine', ...]\n",
        "\n",
        "        Option 2:\n",
        "        [('file', (5, 0)),\n",
        "         ('five', (67, 1)),\n",
        "         ('fire', (54, 1)),\n",
        "         ('fine', (17, 1))...]  \n",
        "        '''\n",
        "\n",
        "    def best_word(self, s, silent=False):\n",
        "        try:\n",
        "            return self.get_suggestions(s, silent)[0]\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, field, start_time=time()):\n",
        "        self.field = field\n",
        "        self.start_time = start_time\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, dataframe):\n",
        "        print(f'[{time()-self.start_time}] select {self.field}')\n",
        "        dt = dataframe[self.field].dtype\n",
        "        if is_categorical_dtype(dt):\n",
        "            return dataframe[self.field].cat.codes[:, None]\n",
        "        elif is_numeric_dtype(dt):\n",
        "            return dataframe[self.field][:, None]\n",
        "        else:\n",
        "            return dataframe[self.field]\n",
        "\n",
        "\n",
        "class DropColumnsByDf(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, min_df=1, max_df=1.0):\n",
        "        self.min_df = min_df\n",
        "        self.max_df = max_df\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        m = X.tocsc()\n",
        "        self.nnz_cols = ((m != 0).sum(axis=0) >= self.min_df).A1\n",
        "        if self.max_df < 1.0:\n",
        "            max_df = m.shape[0] * self.max_df\n",
        "            self.nnz_cols = self.nnz_cols & ((m != 0).sum(axis=0) <= max_df).A1\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        m = X.tocsc()\n",
        "        return m[:, self.nnz_cols]\n",
        "\n",
        "\n",
        "def get_rmsle(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n",
        "\n",
        "\n",
        "def split_cat(text):\n",
        "    try:\n",
        "        cats = text.split(\"/\")\n",
        "        return cats[0], cats[1], cats[2], cats[0] + '/' + cats[1]\n",
        "    except:\n",
        "        print(\"no category\")\n",
        "        return 'other', 'other', 'other', 'other/other'\n",
        "\n",
        "\n",
        "def brands_filling(dataset):\n",
        "    vc = dataset['brand_name'].value_counts()\n",
        "    brands = vc[vc > 0].index\n",
        "    brand_word = r\"[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\n",
        "\n",
        "    many_w_brands = brands[brands.str.contains(' ')]\n",
        "    one_w_brands = brands[~brands.str.contains(' ')]\n",
        "\n",
        "    ss2 = SymSpell(max_edit_distance=0)\n",
        "    ss2.create_dictionary_from_arr(many_w_brands, token_pattern=r'.+')\n",
        "\n",
        "    ss1 = SymSpell(max_edit_distance=0)\n",
        "    ss1.create_dictionary_from_arr(one_w_brands, token_pattern=r'.+')\n",
        "\n",
        "    two_words_re = re.compile(r\"(?=(\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+))\")\n",
        "\n",
        "    def find_in_str_ss2(row):\n",
        "        for doc_word in two_words_re.finditer(row):\n",
        "            print(doc_word)\n",
        "            suggestion = ss2.best_word(doc_word.group(1), silent=True)\n",
        "            if suggestion is not None:\n",
        "                return doc_word.group(1)\n",
        "        return ''\n",
        "\n",
        "    def find_in_list_ss1(list):\n",
        "        for doc_word in list:\n",
        "            suggestion = ss1.best_word(doc_word, silent=True)\n",
        "            if suggestion is not None:\n",
        "                return doc_word\n",
        "        return ''\n",
        "\n",
        "    def find_in_list_ss2(list):\n",
        "        for doc_word in list:\n",
        "            suggestion = ss2.best_word(doc_word, silent=True)\n",
        "            if suggestion is not None:\n",
        "                return doc_word\n",
        "        return ''\n",
        "\n",
        "    print(f\"Before empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
        "\n",
        "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(\n",
        "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
        "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_name]\n",
        "\n",
        "    n_desc = dataset[dataset['brand_name'] == '']['item_description'].str.findall(\n",
        "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
        "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_desc]\n",
        "\n",
        "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(pat=brand_word)\n",
        "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in n_name]\n",
        "\n",
        "    desc_lower = dataset[dataset['brand_name'] == '']['item_description'].str.findall(pat=brand_word)\n",
        "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in desc_lower]\n",
        "\n",
        "    print(f\"After empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
        "\n",
        "    del ss1, ss2\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def preprocess_regex(dataset, start_time=time()):\n",
        "    karats_regex = r'(\\d)([\\s-]?)(karat|karats|carat|carats|kt)([^\\w])'\n",
        "    karats_repl = r'\\1k\\4'\n",
        "\n",
        "    unit_regex = r'(\\d+)[\\s-]([a-z]{2})(\\s)'\n",
        "    unit_repl = r'\\1\\2\\3'\n",
        "\n",
        "    dataset['name'] = dataset['name'].str.replace(karats_regex, karats_repl)\n",
        "    dataset['item_description'] = dataset['item_description'].str.replace(karats_regex, karats_repl)\n",
        "    print(f'[{time() - start_time}] Karats normalized.')\n",
        "\n",
        "    dataset['name'] = dataset['name'].str.replace(unit_regex, unit_repl)\n",
        "    dataset['item_description'] = dataset['item_description'].str.replace(unit_regex, unit_repl)\n",
        "    print(f'[{time() - start_time}] Units glued.')\n",
        "\n",
        "\n",
        "def preprocess_pandas(train, test, start_time=time()):\n",
        "    train = train[train.price > 0.0].reset_index(drop=True)\n",
        "    print('Train shape without zero price: ', train.shape)\n",
        "\n",
        "    nrow_train = train.shape[0]\n",
        "    y_train = np.log1p(train[\"price\"])\n",
        "    merge: pd.DataFrame = pd.concat([train, test])\n",
        "\n",
        "    del train\n",
        "    del test\n",
        "    gc.collect()\n",
        "\n",
        "    merge['has_category'] = (merge['category_name'].notnull()).astype('category')\n",
        "    print(f'[{time() - start_time}] Has_category filled.')\n",
        "\n",
        "    merge['category_name'] = merge['category_name'] \\\n",
        "        .fillna('other/other/other') \\\n",
        "        .str.lower() \\\n",
        "        .astype(str)\n",
        "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'], merge['gen_subcat1'] = \\\n",
        "        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
        "    print(f'[{time() - start_time}] Split categories completed.')\n",
        "\n",
        "    merge['has_brand'] = (merge['brand_name'].notnull()).astype('category')\n",
        "    print(f'[{time() - start_time}] Has_brand filled.')\n",
        "\n",
        "    merge['gencat_cond'] = merge['general_cat'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
        "    merge['subcat_1_cond'] = merge['subcat_1'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
        "    merge['subcat_2_cond'] = merge['subcat_2'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
        "    print(f'[{time() - start_time}] Categories and item_condition_id concancenated.')\n",
        "\n",
        "    merge['name'] = merge['name'] \\\n",
        "        .fillna('') \\\n",
        "        .str.lower() \\\n",
        "        .astype(str)\n",
        "    merge['brand_name'] = merge['brand_name'] \\\n",
        "        .fillna('') \\\n",
        "        .str.lower() \\\n",
        "        .astype(str)\n",
        "    merge['item_description'] = merge['item_description'] \\\n",
        "        .fillna('') \\\n",
        "        .str.lower() \\\n",
        "        .replace(to_replace='No description yet', value='')\n",
        "    print(f'[{time() - start_time}] Missing filled.')\n",
        "\n",
        "    preprocess_regex(merge, start_time)\n",
        "\n",
        "    brands_filling(merge)\n",
        "    print(f'[{time() - start_time}] Brand name filled.')\n",
        "\n",
        "    merge['name'] = merge['name'] + ' ' + merge['brand_name']\n",
        "    print(f'[{time() - start_time}] Name concancenated.')\n",
        "\n",
        "    merge['item_description'] = merge['item_description'] \\\n",
        "                                + ' ' + merge['name'] \\\n",
        "                                + ' ' + merge['subcat_1'] \\\n",
        "                                + ' ' + merge['subcat_2'] \\\n",
        "                                + ' ' + merge['general_cat'] \\\n",
        "                                + ' ' + merge['brand_name']\n",
        "    print(f'[{time() - start_time}] Item description concatenated.')\n",
        "\n",
        "    merge.drop(['price', 'test_id', 'train_id'], axis=1, inplace=True)\n",
        "\n",
        "    return merge, y_train, nrow_train\n",
        "\n",
        "\n",
        "def intersect_drop_columns(train: csr_matrix, valid: csr_matrix, min_df=0):\n",
        "    t = train.tocsc()\n",
        "    v = valid.tocsc()\n",
        "    nnz_train = ((t != 0).sum(axis=0) >= min_df).A1\n",
        "    nnz_valid = ((v != 0).sum(axis=0) >= min_df).A1\n",
        "    nnz_cols = nnz_train & nnz_valid\n",
        "    res = t[:, nnz_cols], v[:, nnz_cols]\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mp.set_start_method('forkserver', True)\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    train = pd.read_table(os.path.join(INPUT_PATH, 'train.tsv'),\n",
        "                          engine='c',\n",
        "                          dtype={'item_condition_id': 'category',\n",
        "                                 'shipping': 'category'}\n",
        "                          )\n",
        "    test = pd.read_table(os.path.join(INPUT_PATH, 'test.tsv'),\n",
        "                         engine='c',\n",
        "                         dtype={'item_condition_id': 'category',\n",
        "                                'shipping': 'category'}\n",
        "                         )\n",
        "    print(f'[{time() - start_time}] Finished to load data')\n",
        "    print('Train shape: ', train.shape)\n",
        "    print('Test shape: ', test.shape)\n",
        "\n",
        "    submission: pd.DataFrame = test[['test_id']]\n",
        "\n",
        "    merge, y_train, nrow_train = preprocess_pandas(train, test, start_time)\n",
        "\n",
        "    meta_params = {'name_ngram': (1, 2),\n",
        "                   'name_max_f': 75000,\n",
        "                   'name_min_df': 10,\n",
        "\n",
        "                   'category_ngram': (2, 3),\n",
        "                   'category_token': '.+',\n",
        "                   'category_min_df': 10,\n",
        "\n",
        "                   'brand_min_df': 10,\n",
        "\n",
        "                   'desc_ngram': (1, 3),\n",
        "                   'desc_max_f': 150000,\n",
        "                   'desc_max_df': 0.5,\n",
        "                   'desc_min_df': 10}\n",
        "\n",
        "    stopwords = frozenset(['the', 'a', 'an', 'is', 'it', 'this', ])\n",
        "    # 'i', 'so', 'its', 'am', 'are'])\n",
        "\n",
        "    vectorizer = FeatureUnion([\n",
        "        ('name', Pipeline([\n",
        "            ('select', ItemSelector('name', start_time=start_time)),\n",
        "            ('transform', HashingVectorizer(\n",
        "                ngram_range=(1, 2),\n",
        "                n_features=2 ** 27,\n",
        "                norm='l2',\n",
        "                lowercase=False,\n",
        "                stop_words=stopwords\n",
        "            )),\n",
        "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
        "        ])),\n",
        "        ('category_name', Pipeline([\n",
        "            ('select', ItemSelector('category_name', start_time=start_time)),\n",
        "            ('transform', HashingVectorizer(\n",
        "                ngram_range=(1, 1),\n",
        "                token_pattern='.+',\n",
        "                tokenizer=split_cat,\n",
        "                n_features=2 ** 27,\n",
        "                norm='l2',\n",
        "                lowercase=False\n",
        "            )),\n",
        "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
        "        ])),\n",
        "        ('brand_name', Pipeline([\n",
        "            ('select', ItemSelector('brand_name', start_time=start_time)),\n",
        "            ('transform', CountVectorizer(\n",
        "                token_pattern='.+',\n",
        "                min_df=2,\n",
        "                lowercase=False\n",
        "            )),\n",
        "        ])),\n",
        "        ('gencat_cond', Pipeline([\n",
        "            ('select', ItemSelector('gencat_cond', start_time=start_time)),\n",
        "            ('transform', CountVectorizer(\n",
        "                token_pattern='.+',\n",
        "                min_df=2,\n",
        "                lowercase=False\n",
        "            )),\n",
        "        ])),\n",
        "        ('subcat_1_cond', Pipeline([\n",
        "            ('select', ItemSelector('subcat_1_cond', start_time=start_time)),\n",
        "            ('transform', CountVectorizer(\n",
        "                token_pattern='.+',\n",
        "                min_df=2,\n",
        "                lowercase=False\n",
        "            )),\n",
        "        ])),\n",
        "        ('subcat_2_cond', Pipeline([\n",
        "            ('select', ItemSelector('subcat_2_cond', start_time=start_time)),\n",
        "            ('transform', CountVectorizer(\n",
        "                token_pattern='.+',\n",
        "                min_df=2,\n",
        "                lowercase=False\n",
        "            )),\n",
        "        ])),\n",
        "        ('has_brand', Pipeline([\n",
        "            ('select', ItemSelector('has_brand', start_time=start_time)),\n",
        "            ('ohe', OneHotEncoder())\n",
        "        ])),\n",
        "        ('shipping', Pipeline([\n",
        "            ('select', ItemSelector('shipping', start_time=start_time)),\n",
        "            ('ohe', OneHotEncoder())\n",
        "        ])),\n",
        "        ('item_condition_id', Pipeline([\n",
        "            ('select', ItemSelector('item_condition_id', start_time=start_time)),\n",
        "            ('ohe', OneHotEncoder())\n",
        "        ])),\n",
        "        ('item_description', Pipeline([\n",
        "            ('select', ItemSelector('item_description', start_time=start_time)),\n",
        "            ('hash', HashingVectorizer(\n",
        "                ngram_range=(1, 3),\n",
        "                n_features=2 ** 27,\n",
        "                dtype=np.float32,\n",
        "                norm='l2',\n",
        "                lowercase=False,\n",
        "                stop_words=stopwords\n",
        "            )),\n",
        "            ('drop_cols', DropColumnsByDf(min_df=2)),\n",
        "        ]))\n",
        "    ], n_jobs=1)\n",
        "\n",
        "    sparse_merge = vectorizer.fit_transform(merge)\n",
        "    print(f'[{time() - start_time}] Merge vectorized')\n",
        "    print(sparse_merge.shape)\n",
        "\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "    X = tfidf_transformer.fit_transform(sparse_merge)\n",
        "    print(f'[{time() - start_time}] TF/IDF completed')\n",
        "\n",
        "    X_train = X[:nrow_train]\n",
        "    print(X_train.shape)\n",
        "\n",
        "    X_test = X[nrow_train:]\n",
        "    del merge\n",
        "    del sparse_merge\n",
        "    del vectorizer\n",
        "    del tfidf_transformer\n",
        "    gc.collect()\n",
        "\n",
        "    X_train, X_test = intersect_drop_columns(X_train, X_test, min_df=1)\n",
        "    print(f'[{time() - start_time}] Drop only in train or test cols: {X_train.shape[1]}')\n",
        "    gc.collect()\n",
        "\n",
        "    ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
        "    ridge.fit(X_train, y_train)\n",
        "    print(f'[{time() - start_time}] Train Ridge completed. Iterations: {ridge.n_iter_}')\n",
        "\n",
        "    predsR = ridge.predict(X_test)\n",
        "    print(f'[{time() - start_time}] Predict Ridge completed.')\n",
        "\n",
        "    submission.loc[:, 'price'] = np.expm1(predsR)\n",
        "    submission.loc[submission['price'] < 0.0, 'price'] = 0.0\n",
        "    submission.to_csv(\"submission_ridge.csv\", index=False)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12.847861051559448] Finished to load data\n",
            "Train shape:  (1482535, 8)\n",
            "Test shape:  (693359, 7)\n",
            "Train shape without zero price:  (1481661, 8)\n",
            "[13.717209339141846] Has_category filled.\n",
            "[22.52828288078308] Split categories completed.\n",
            "[22.647936820983887] Has_brand filled.\n",
            "[26.38297128677368] Categories and item_condition_id concancenated.\n",
            "[31.442798376083374] Missing filled.\n",
            "[44.837557554244995] Karats normalized.\n",
            "[64.62334752082825] Units glued.\n",
            "total words processed: 2671\n",
            "total unique words in corpus: 2671\n",
            "total items in dictionary (corpus words and deletions): 2671\n",
            "  edit distance for deletions: 0\n",
            "  length of longest word in corpus: 39\n",
            "total words processed: 2616\n",
            "total unique words in corpus: 2616\n",
            "total items in dictionary (corpus words and deletions): 2616\n",
            "  edit distance for deletions: 0\n",
            "  length of longest word in corpus: 15\n",
            "Before empty brand_name: 927861\n",
            "After empty brand_name: 252719\n",
            "[118.67165637016296] Brand name filled.\n",
            "[119.91507935523987] Name concancenated.\n",
            "[128.04528379440308] Item description concatenated.\n",
            "[128.69582748413086] select name\n",
            "[157.70457220077515] select category_name\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[169.48336815834045] select brand_name\n",
            "[176.14824962615967] select gencat_cond\n",
            "[182.90946888923645] select subcat_1_cond\n",
            "[189.54591727256775] select subcat_2_cond\n",
            "[196.38384675979614] select has_brand\n",
            "[196.57394647598267] select shipping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:332: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:332: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[196.76185750961304] select item_condition_id\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:332: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[196.97480964660645] select item_description\n",
            "[414.5241434574127] Merge vectorized\n",
            "(2175020, 8961796)\n",
            "[536.4496014118195] TF/IDF completed\n",
            "(1481661, 8961796)\n",
            "[603.9689340591431] Drop only in train or test cols: 5976503\n",
            "[2139.88228225708] Train Ridge completed. Iterations: None\n",
            "[2140.3393836021423] Predict Ridge completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[key] = _infer_fill_value(value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n"
          ]
        }
      ]
    }
  ]
}