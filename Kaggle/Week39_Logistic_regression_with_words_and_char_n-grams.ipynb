{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WEEK39_Logistic_regression_with_words_and_char_n-grams.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1aCWzcd8HuN4jMdadTTpcVLR6JS4WQI0B","authorship_tag":"ABX9TyNcuZVTX4vq/c/+oZlA+6iz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## [Kaggle Clone Coding] Toxic Comment Classification Challenge\n","- [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n","- [Logistic regression with words and char n-grams](https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams)\n","  \n","- Task : 댓글 유형의 독성 확률을 예측\n","---"],"metadata":{"id":"7jr6G7mEGNYp"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IB1wjFL1GI20","executionInfo":{"status":"ok","timestamp":1639892298804,"user_tz":-540,"elapsed":1490316,"user":{"displayName":"이윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08003539664697596279"}},"outputId":"e7a5999d-4db1-492a-cd3b-e28cff148cc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Kaggle/kaggle\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:539: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  \"The parameter 'stop_words' will not be used\"\n"]},{"output_type":"stream","name":"stdout","text":["CV score for class toxic is 0.9692180496733993\n","CV score for class severe_toxic is 0.9875920222437825\n","CV score for class obscene is 0.9838683536564505\n","CV score for class threat is 0.9833772508491236\n","CV score for class insult is 0.9774236228165338\n","CV score for class identity_hate is 0.9739429245526633\n","Total CV score is 0.9792370372986587\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","from scipy.sparse import hstack\n","\n","%cd /content/drive/MyDrive/Kaggle/kaggle/\n","\n","class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","\n","train = pd.read_csv('train.csv').fillna(' ')\n","test = pd.read_csv('test.csv').fillna(' ')\n","\n","train_text = train['comment_text']\n","test_text = test['comment_text']\n","all_text = pd.concat([train_text, test_text])\n","\n","word_vectorizer = TfidfVectorizer(\n","    sublinear_tf=True,\n","    strip_accents='unicode',\n","    analyzer='word',\n","    token_pattern=r'\\w{1,}',\n","    stop_words='english',\n","    ngram_range=(1, 1),\n","    max_features=10000)\n","word_vectorizer.fit(all_text)\n","train_word_features = word_vectorizer.transform(train_text)\n","test_word_features = word_vectorizer.transform(test_text)\n","\n","char_vectorizer = TfidfVectorizer(\n","    sublinear_tf=True,\n","    strip_accents='unicode',\n","    analyzer='char',\n","    stop_words='english',\n","    ngram_range=(2, 6),\n","    max_features=50000)\n","char_vectorizer.fit(all_text)\n","train_char_features = char_vectorizer.transform(train_text)\n","test_char_features = char_vectorizer.transform(test_text)\n","\n","train_features = hstack([train_char_features, train_word_features])\n","test_features = hstack([test_char_features, test_word_features])\n","\n","scores = []\n","submission = pd.DataFrame.from_dict({'id': test['id']})\n","for class_name in class_names:\n","    train_target = train[class_name]\n","    classifier = LogisticRegression(C=0.1, solver='sag')\n","\n","    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n","    scores.append(cv_score)\n","    print('CV score for class {} is {}'.format(class_name, cv_score))\n","\n","    classifier.fit(train_features, train_target)\n","    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n","\n","print('Total CV score is {}'.format(np.mean(scores)))\n","\n","submission.to_csv('submission.csv', index=False)"]}]}