{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특징\n",
    "    1. Classification과 Regression Task 모두 가능하다. \n",
    "    2. Random Forest의 기본구성요소\n",
    "    3. sklearn의 `export_graphviz` 함수를 통해 결정 트리를 시각화 할 수 있다.\n",
    "    \n",
    "![image](https://user-images.githubusercontent.com/69336270/127769618-dbf55f05-bfec-41b6-b399-71c262a4db20.png)\n",
    "\n",
    "> Sklearn의 경우 CART 알고리즘만 사용하므로 이진 트리만 만들어 지지만, ID3와 같은 알고리즘은 2개 이상의 자식 노드를 가지는 결정 트리를 만들 수 있다.\n",
    "<br>\n",
    "- Classificaton 과정  \n",
    "    1. 들어온 데이터는 Root Node를 통과한다. 이때, Root Node에 할당된 조건에 따라 True의 경우 Left Child Node로 False의 경우 Right Child Node로 이동한다. (depth = 1)  \n",
    "    2. Root Node의 조건에 따라 depth = 1인 노드로 이동한 다음, 해당 노드가 Leaf Node (단말 노드)인 경우 추가적인 검사 없이 이동된 노드의 class로 예측한다.  \n",
    "    3. Root Node의 조건에 따라 depth = 1인 노드로 이동한 다음, 해당 노드가 Leaf Node (단말 노드)가 아닌 경우 해당 노드에 할당된 조건에 따라 추가적으로 이동한다. \n",
    "    \n",
    "> 의사결정나무 알고리즘과 같이 직관적이고 결정방식을 이해하기 쉬운 모델을 `화이트 박스` 모델이라고 칭한다. 반대로, 랜덤 포레스트 알고리즘이나 신경망 모델과 같이 어떠한 예측을 왜 그렇게 만드는 지 설명하기 어려운 모델을 `블랙 박스` 모델이라고 칭한다. 블랙 박스는 뛰어난 성능에 비하여 판단의 근거를 파악하기 어렵다는 한계를 지닌다. 최근 이러한 블랙 박스 모델의 한계점을 해결하기 위해 `XAI(설명 가능한 인공지능)`이 각광받고 있다. \n",
    "<br><br>\n",
    "- 속성\n",
    "    - sample : 얼마나 많은 훈련 샘플이 적용되었지에 대한 값으로 해당 노드를 통과한 데이터의 수라고 볼 수 있다. \n",
    "    - value : 해당 노드를 통과한 데이터에 대하여 각 클래스 별 데이터의 수라고 볼 수 있다. 따라서, 하위 노드들의 각 class별 value 값을 더하면 해당 노드의 각 class별 value 값이 된다.\n",
    "    - gini : 각 노드를 통과한 데이터의 class에 대한 지표로 `불순도`를 의미한다. 즉, gini 계수가 클 수록 데이터가 많이 분산됨을 의미한다. \n",
    "     예를 들어, Root Node의 Left Child Node(주황색)의 경우 첫번째 class에 대한 훈련 샘플을 가지고 있으므로 순수 노드이며 gini = 0이다. 추가로 depth = 2인 Left Node(초록색)의 경우 gini = $1-(0/54)^2 - (49/54)^2 - (5/54)^2 \\approx 0.168$\n",
    "> $G_i = 1-\\sum_{k=1}^{n}p_{i, k}^2$,   (이때, $p_{i, k}$는 i번째 노드에 있는 훈련 샘플 중 클래스 k에 속한 샘플의 비율)\n",
    "    - max_depth : 의사 결정 나무의 최대 깊이로 모델 구축 시 parameter로 설정할 수 있다.\n",
    "<br><br>\n",
    "- Class 확률  \n",
    "    한 샘플이 특정 클래스 k에 속할 확률 = 해당 샘플에 대한 leaf node의 클래스 k의 훈련 샘플 비율\n",
    "<br><br>\n",
    "- CART 알고리즘  \n",
    "    Sklearn의 경우 CART 알고리즘을 통해 의사결정나무를 훈련시킨다. 이때, CART 알고리즘은 클래스 비율을 고려하여 가장 순수하게 subset을 나눌 수 있는 feature와 임계값 $t_k$를 통해 2개의 subset으로 나눈다. 해당 과정은 최대 깊이(max_depth)에 도달하거나 gini를 줄이는 분할을 찾을 수 없을 때까지 진행된다. 다만, CART 알고리즘은 Greedy 알고리즘이므로 최적의 솔루션이라고는 볼 수 없다. \n",
    "<br><br>\n",
    "- Entropy  \n",
    "![image](https://user-images.githubusercontent.com/69336270/127770943-619b5f12-bf53-4c90-8080-4ff1f9b8af91.png)  \n",
    "\n",
    "    gini 계수 이외도 `Entropy` 역시 불순도를 측정하는 지표이다. 두 지표 모두 큰 차이는 없으나, gini 계수가 가장 빈도가 높은 class를 한쪽 가지로 고립시키는 경향이 있는 반면 엔트로피는 조금 더 균형된 트리를 구축한다. \n",
    "    > $H_i = \\sum_{k=1}^{n}p_{i, k} log_2(p_{i, k})$\n",
    "\n",
    "- 규제 파라미터  \n",
    "    데이터의 선형성을 가정하는 선형회귀모델과 달리 의사결정나무는 자유도가 매우 크다. 따라서, 훈련 데이터에 대한 과대적합을 피하고, 모델의 자유도를 제한하기 위해 규제 파라미터가 필요하다. 이때, 파라미터의 전치사가 min인 경우 파라미터 값을 증가시키고, 전치사가 max인 경우 파라미터 값을 감소시킬 수록 규제가 커진다.\n",
    "    - min_samples_leaf : 리프 노드가 가져야할 최소 샘플 수\n",
    "    - min_weight_graction_leaf : min_samples_leaf와 같지만, 가중치가 부여된 전체 데이터 수에서의 비율\n",
    "    - max_leaf_nodes : 리프 노드의 최대 수\n",
    "    - max_features : 각 노드에서 분할에 사용할 특성의 최대 수\n",
    "<br><br>\n",
    "- Regression\n",
    "    ![image](https://user-images.githubusercontent.com/69336270/127770968-fe67d5ee-1ff6-4c17-92e8-50bc079eecc7.png)  \n",
    "\n",
    "    classification과 달리 샘플의 target 값을 예측한다. 이때, 예측 값은 해당 leaf node에 있는 훈련 샘플들의 평균 target 값이다. regression의 경우 gini 계수 혹은 entropy를 지표로 삼던 Classification과 달리 `MSE`를 지표로 삼는다. \n",
    "    \n",
    "    ![image](https://user-images.githubusercontent.com/69336270/127771047-7c608516-5b7c-4864-aca2-e3abc89886cf.png)\n",
    "    왼쪽 그래프는 max_depth = 2인 의사결정나무, 오른쪽 그래프는 max_depth = 3인 의사결정나무이다. \n",
    "<br><br>\n",
    "- 한계점   \n",
    "    ![image](https://user-images.githubusercontent.com/69336270/127771111-8134bbec-4278-4ceb-8619-0b7aad872c1a.png)\n",
    "     의사결정나무의 경우 데이터의 회전과 같은 작은 변화나 세부사항에도 민감하게 반응한다는 단점을 지닌다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
